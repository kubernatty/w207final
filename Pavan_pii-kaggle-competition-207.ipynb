{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":7803679,"sourceType":"datasetVersion","datasetId":4340749},{"sourceId":8160481,"sourceType":"datasetVersion","datasetId":4827926},{"sourceId":160550662,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T23:02:30.707119Z","iopub.execute_input":"2024-04-18T23:02:30.707976Z","iopub.status.idle":"2024-04-18T23:02:48.846038Z","shell.execute_reply.started":"2024-04-18T23:02:30.707938Z","shell.execute_reply":"2024-04-18T23:02:48.845053Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:02:48.847840Z","iopub.execute_input":"2024-04-18T23:02:48.848461Z","iopub.status.idle":"2024-04-18T23:02:48.972125Z","shell.execute_reply.started":"2024-04-18T23:02:48.848428Z","shell.execute_reply":"2024-04-18T23:02:48.971306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahoy there, me hearties! Let me spin ye a yarn about this here code, known as 'tokenize':\n\n- **Function Purpose**: This here 'tokenize' function be a vital part of our quest in the world of Natural Language Processing (NLP). Its mission be to take an 'example' and a 'tokenizer,' then turn that text into tokens while keepin' tabs on where they belong.\n\n- **Initialization**: We start by creatin' two lists - 'text' and 'token_map' - to store the tokens and their positions.\n\n- **Tracking Tokens**: The 'idx' variable sets sail at 0 and helps us navigate through the tokens as we process 'em.\n\n- **Token Loop**: We voyage through the tokens and their trailin' white spaces (if any). For each token 't' and its mate, the whitespace 'ws':\n\n    - 't' gets added to the 'text' list so we can gather all the tokens in one place.\n    \n    - 'token_map' expands by repeatin' 'idx' as many times as there be characters in token 't.' This step be our map to show where each character belongs to which token.\n    \n    - If there be trailin' whitespace ('ws'), we add a space to 'text' and put a '-1' in 'token_map' to mark where the whitespace lies.\n\n    - 'idx' gets a boost to be ready for the next token on our journey.\n\n- **Tokenization**: The 'text' we've gathered be tokenized using the 'tokenizer' we were given. We also ask for offsets mappings and set a maximum token length ('max_length') to make sure our tokens don't get too unruly.\n\n- **Returning the Result**: The function hands back a chest of treasures in the form of a dictionary. Inside, ye'll find the tokenized data, includin' offset mappings, and the 'token_map.' This be essential for future NLP adventures where knowin' where the tokens sit be key.\n\nThis 'tokenize' function be our loyal mate on the high seas of NLP, helpin' us find the buried treasure of information within the text. Aye, it be a treasure map for the crew! 🏴‍☠️💰\n","metadata":{}},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 2500\n\ndef tokenize(example, tokenizer):\n    # We be creatin' two empty lists, 'text' and 'token_map', to store our tokens and their respective maps.\n    text = []\n    token_map = []\n    \n    # We start the 'idx' at 0, it be used to keep track of the tokens.\n    idx = 0\n    \n    # Now, we be loopin' through the tokens and their trailin' white spaces.\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        # We add the token 't' to the 'text' list.\n        text.append(t)\n        \n        # We be extendin' the 'token_map' list by repeatin' the 'idx' as many times as the length of token 't'.\n        token_map.extend([idx]*len(t))\n        \n        # If there be trailin' whitespace (ws), we add a space to 'text' and mark it with a '-1' in 'token_map'.\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        # We increment 'idx' to keep track of the next token.\n        idx += 1\n        \n    # Now, we tokenize the concatenated 'text' and return offsets mappings along with 'token_map'.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    \n    # We return a dictionary containin' the tokenized data and the 'token_map'.\n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:02:48.973480Z","iopub.execute_input":"2024-04-18T23:02:48.973753Z","iopub.status.idle":"2024-04-18T23:02:48.981288Z","shell.execute_reply.started":"2024-04-18T23:02:48.973728Z","shell.execute_reply":"2024-04-18T23:02:48.980195Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Ahoy there, matey! Allow me to decipher this code for ye:\n\n- **Loading Test Data**: We begin by loading test data from a JSON file, setting sail on our data adventure.\n\n- **Creating a Dataset**: Next, we construct a dataset using the data we've loaded. This dataset be our treasure chest, containing details like 'full_text,' 'document,' 'tokens,' and 'trailing_whitespace.'\n\n- **Tokenizer and Model**: We call upon the services of a trusty 'tokenizer' and a fearsome 'model' to aid us in our task. These tools be crucial for handling text data.\n\n- **Parallel Tokenization**: We employ parallel processing to tokenize our dataset using the 'tokenize' function, ensuring speedy execution.\n\n- **Data Collator for Classification**: A 'data collator' be created to assist in token classification, ensuring everything lines up nicely.\n\n- **Training Arguments**: We set the stage with training arguments, specifyin' the output directory, evaluation batch size, and a vow of silence – we won't be reportin' results to any server.\n\n- **Trainer for Evaluation**: Finally, we muster the 'trainer' – a commander for our model, equipped with the model itself, training arguments, data collator, and tokenizer.\n\nWith this code, we be ready to evaluate our model's performance on the test data, settin' sail for an exciting NLP adventure! 🏴‍☠️🌊\n","metadata":{}},{"cell_type":"code","source":"# JUST 1 MODEL INSTEAD OF 3\n\n# Load the test data from a JSON file\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n#X_train, X_val, y_train, y_val = train_test_split(train_df['token_str'], train_df['label'], test_size=0.2, random_state=42)\n\n# Create a dataset from the loaded data\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\n# Initialize a tokenizer and model from the pretrained model path\nmodel_paths = {'/kaggle/input/pii-deberta-models/cola-de-piiranha' : 1/3,\n              '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha' : 1/3,\n              '/kaggle/input/pii-deberta-models/cabeza-de-piiranha' : 1/3}\n\nmodel = '/kaggle/input/pii-deberta-models/cola-de-piiranha'\n\n# first_model_path = list(model_paths.keys())[0]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\n# Tokenize the dataset using the 'tokenize' function in parallel\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 4)\n\n\nimport gc\nimport torch\nimport numpy as np\n\nfrom scipy.special import softmax\n\n\n# all_preds = []\n\n# Calculate the total weight\n# total_weight = sum(model_paths.values())\n\n\n# print(\"begin tokenization/training on {} model\".format(model_path))\n# tokenizer = AutoTokenizer.from_pretrained(model)\n\nmodel = AutoModelForTokenClassification.from_pretrained(model)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)\ntrainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\nprint(\"corresponding labels b:\", model.config.id2label)\n# print(\"begin predictions on {} model\".format(model_path))\npredictions = trainer.predict(ds).predictions\n    # This idea from this notebook: https://www.kaggle.com/code/olyatsimboy/912-blending-0-903-0-854-deberta3base\n#     weighted_predictions = softmax(predictions, axis = -1) * weight # softmax helps convert the logits (the raw, unnormalized scores outputted by the last layer of the model) to probabilities\n#     all_preds.append(weighted_predictions)\nprint(\"corresponding labels:\", model.config.id2label)\ndel model, trainer\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Calculate the weighted average of predictions\n# weighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\nweighted_average_predictions = predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:02:48.983803Z","iopub.execute_input":"2024-04-18T23:02:48.984091Z","iopub.status.idle":"2024-04-18T23:03:08.837690Z","shell.execute_reply.started":"2024-04-18T23:02:48.984067Z","shell.execute_reply":"2024-04-18T23:03:08.836622Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"830d73e0ac5b49049f55a383c824b84e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c3f0a0b63e436d8870e9f894e15e65"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/2 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d79ef692edb4abfb56d9ab69ef14f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/2 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf0398d84d9425daee30d8a03dbee1d"}},"metadata":{}},{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"corresponding labels b: {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"corresponding labels: {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# WEIGHTED AVERAGE OF THE 3 MODELS\n'''\n# Load the test data from a JSON file\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n#X_train, X_val, y_train, y_val = train_test_split(train_df['token_str'], train_df['label'], test_size=0.2, random_state=42)\n\n# Create a dataset from the loaded data\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\n# Initialize a tokenizer and model from the pretrained model path\nmodel_paths = {'/kaggle/input/pii-deberta-models/cola-de-piiranha' : 1/3,\n              '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha' : 1/3,\n              '/kaggle/input/pii-deberta-models/cabeza-de-piiranha' : 1/3}\n\nfirst_model_path = list(model_paths.keys())[0]\n\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n\n# Tokenize the dataset using the 'tokenize' function in parallel\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 4)\n\n\nimport gc\nimport torch\nimport numpy as np\n\nfrom scipy.special import softmax\n\n\nall_preds = []\n\n# Calculate the total weight\ntotal_weight = sum(model_paths.values())\n\nfor model_path, weight in model_paths.items():\n    print(\"begin tokenization/training on {} model\".format(model_path))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model = AutoModelForTokenClassification.from_pretrained(model_path)\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 16)\n    args = TrainingArguments(\n        \".\", \n        per_device_eval_batch_size=1, \n        report_to=\"none\",\n    )\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=collator, \n        tokenizer=tokenizer,\n    )\n    print(\"corresponding labels b:\", model.config.id2label)\n    print(\"begin predictions on {} model\".format(model_path))\n    predictions = trainer.predict(ds).predictions\n    # This idea from this notebook: https://www.kaggle.com/code/olyatsimboy/912-blending-0-903-0-854-deberta3base\n    weighted_predictions = softmax(predictions, axis = -1) * weight # softmax helps convert the logits (the raw, unnormalized scores outputted by the last layer of the model) to probabilities\n    all_preds.append(weighted_predictions)\n    print(\"corresponding labels:\", model.config.id2label)\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# Calculate the weighted average of predictions\nweighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.838913Z","iopub.execute_input":"2024-04-18T23:03:08.839178Z","iopub.status.idle":"2024-04-18T23:03:08.848790Z","shell.execute_reply.started":"2024-04-18T23:03:08.839153Z","shell.execute_reply":"2024-04-18T23:03:08.847767Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\n# Load the test data from a JSON file\\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\\n#X_train, X_val, y_train, y_val = train_test_split(train_df[\\'token_str\\'], train_df[\\'label\\'], test_size=0.2, random_state=42)\\n\\n# Create a dataset from the loaded data\\nds = Dataset.from_dict({\\n    \"full_text\": [x[\"full_text\"] for x in data],\\n    \"document\": [x[\"document\"] for x in data],\\n    \"tokens\": [x[\"tokens\"] for x in data],\\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\\n})\\n\\n# Initialize a tokenizer and model from the pretrained model path\\nmodel_paths = {\\'/kaggle/input/pii-deberta-models/cola-de-piiranha\\' : 1/3,\\n              \\'/kaggle/input/pii-deberta-models/cuerpo-de-piiranha\\' : 1/3,\\n              \\'/kaggle/input/pii-deberta-models/cabeza-de-piiranha\\' : 1/3}\\n\\nfirst_model_path = list(model_paths.keys())[0]\\n\\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\\n\\n# Tokenize the dataset using the \\'tokenize\\' function in parallel\\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 4)\\n\\n\\nimport gc\\nimport torch\\nimport numpy as np\\n\\nfrom scipy.special import softmax\\n\\n\\nall_preds = []\\n\\n# Calculate the total weight\\ntotal_weight = sum(model_paths.values())\\n\\nfor model_path, weight in model_paths.items():\\n    print(\"begin tokenization/training on {} model\".format(model_path))\\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\\n\\n    model = AutoModelForTokenClassification.from_pretrained(model_path)\\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 16)\\n    args = TrainingArguments(\\n        \".\", \\n        per_device_eval_batch_size=1, \\n        report_to=\"none\",\\n    )\\n    trainer = Trainer(\\n        model=model, \\n        args=args, \\n        data_collator=collator, \\n        tokenizer=tokenizer,\\n    )\\n    print(\"corresponding labels b:\", model.config.id2label)\\n    print(\"begin predictions on {} model\".format(model_path))\\n    predictions = trainer.predict(ds).predictions\\n    # This idea from this notebook: https://www.kaggle.com/code/olyatsimboy/912-blending-0-903-0-854-deberta3base\\n    weighted_predictions = softmax(predictions, axis = -1) * weight # softmax helps convert the logits (the raw, unnormalized scores outputted by the last layer of the model) to probabilities\\n    all_preds.append(weighted_predictions)\\n    print(\"corresponding labels:\", model.config.id2label)\\n    del model, trainer\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n\\n# Calculate the weighted average of predictions\\nweighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\\n'"},"metadata":{}}]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.850155Z","iopub.execute_input":"2024-04-18T23:03:08.850699Z","iopub.status.idle":"2024-04-18T23:03:08.865628Z","shell.execute_reply.started":"2024-04-18T23:03:08.850663Z","shell.execute_reply":"2024-04-18T23:03:08.864696Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(10, 1936, 13)"},"metadata":{}}]},{"cell_type":"code","source":"weighted_average_predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.866721Z","iopub.execute_input":"2024-04-18T23:03:08.866988Z","iopub.status.idle":"2024-04-18T23:03:08.876655Z","shell.execute_reply.started":"2024-04-18T23:03:08.866958Z","shell.execute_reply":"2024-04-18T23:03:08.875797Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(10, 1936, 13)"},"metadata":{}}]},{"cell_type":"code","source":"predictions[0][0].sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.879565Z","iopub.execute_input":"2024-04-18T23:03:08.879853Z","iopub.status.idle":"2024-04-18T23:03:08.888548Z","shell.execute_reply.started":"2024-04-18T23:03:08.879829Z","shell.execute_reply":"2024-04-18T23:03:08.887648Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"-6.2648153"},"metadata":{}}]},{"cell_type":"code","source":"softmax_predict = softmax(predictions, axis = -1)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.889838Z","iopub.execute_input":"2024-04-18T23:03:08.890516Z","iopub.status.idle":"2024-04-18T23:03:08.901946Z","shell.execute_reply.started":"2024-04-18T23:03:08.890483Z","shell.execute_reply":"2024-04-18T23:03:08.901115Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"softmax_predict[0][0].sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.905475Z","iopub.execute_input":"2024-04-18T23:03:08.905817Z","iopub.status.idle":"2024-04-18T23:03:08.911657Z","shell.execute_reply.started":"2024-04-18T23:03:08.905793Z","shell.execute_reply":"2024-04-18T23:03:08.910772Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1.0000001"},"metadata":{}}]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.912780Z","iopub.execute_input":"2024-04-18T23:03:08.913105Z","iopub.status.idle":"2024-04-18T23:03:08.921805Z","shell.execute_reply.started":"2024-04-18T23:03:08.913074Z","shell.execute_reply":"2024-04-18T23:03:08.920883Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'token_map'],\n    num_rows: 10\n})"},"metadata":{}}]},{"cell_type":"code","source":"###\n#training_data = pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/train.json')\n#print(training_data.head(5))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.922867Z","iopub.execute_input":"2024-04-18T23:03:08.923159Z","iopub.status.idle":"2024-04-18T23:03:08.931561Z","shell.execute_reply.started":"2024-04-18T23:03:08.923135Z","shell.execute_reply":"2024-04-18T23:03:08.930729Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"'''\nThis post-processing step allows for adjusting the predictions based on the model's confidence in predicting the 'O' class. \nBy using a threshold, you can fine-tune the trade-off between precision and recall for the 'O' class prediction.\n'''\nmodel_path = '/kaggle/input/pii-deberta-models/cola-de-piiranha'\n\nconfig = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\npreds = weighted_average_predictions.argmax(-1) # argmax returns the indices of the maximum value in each row\npreds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\nO_preds = weighted_average_predictions[:,:,12]\n# Change this threshold to \"manually\" adjust for the FBeta metric\nthreshold = 0.9875\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.932653Z","iopub.execute_input":"2024-04-18T23:03:08.932906Z","iopub.status.idle":"2024-04-18T23:03:08.945510Z","shell.execute_reply.started":"2024-04-18T23:03:08.932883Z","shell.execute_reply":"2024-04-18T23:03:08.944435Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model_path","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.946591Z","iopub.execute_input":"2024-04-18T23:03:08.946928Z","iopub.status.idle":"2024-04-18T23:03:08.957174Z","shell.execute_reply.started":"2024-04-18T23:03:08.946893Z","shell.execute_reply":"2024-04-18T23:03:08.956148Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/pii-deberta-models/cola-de-piiranha'"},"metadata":{}}]},{"cell_type":"code","source":"# config files are the same for all the 3 models\nconfig1 = json.load(open(Path('/kaggle/input/pii-deberta-models/cabeza-de-piiranha') / \"config.json\")) \nconfig2 = json.load(open(Path('/kaggle/input/pii-deberta-models/cuerpo-de-piiranha') / \"config.json\"))\nconfig1 == config2","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.958448Z","iopub.execute_input":"2024-04-18T23:03:08.958795Z","iopub.status.idle":"2024-04-18T23:03:08.982457Z","shell.execute_reply.started":"2024-04-18T23:03:08.958764Z","shell.execute_reply":"2024-04-18T23:03:08.981627Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"🏴‍☠️ Ahoy, me shipmates! Let's unravel the secrets of this code:\n\n- **Triplet Gathering**: Our quest begins with the formation of empty lists to store triplets – valuable loot from the dataset. These triplets consist of a 'label,' 'token_id,' and 'token_str.'\n\n- **Data Exploration Voyage**: We embark on a voyage through the dataset, exploring each prediction ('p'), token mapping ('token_map'), offsets ('offsets'), tokens ('tokens'), and documents ('doc').\n\n- **Treasure in Tokens**: For each prediction, we inspect the tokens and their corresponding offsets. We decipher the 'label_pred' – the predicted label from the token.\n\n- **Navigating the Indices**: We navigate the tricky indices, checking for special cases where the start and end indices sum to zero, or where token mapping is -1.\n\n- **Ignore the Whitespace**: We're savvy enough to ignore leading whitespace tokens (\"\\n\\n\") as we seek the real treasure.\n\n- **Forming Triplets**: With the pieces in place, we form a triplet consisting of the 'label_pred,' 'token_id,' and 'token_str.' But, we only add it to our treasure chest ('triplets') if it hasn't been plundered before.\n\n- **Successful Voyage**: Our data exploration voyage yields a bounty of valuable triplets, ready to be analyzed and deciphered!\n\nMay these code treasures guide ye to the heart of the dataset! 🏴‍☠️💰\n","metadata":{}},{"cell_type":"code","source":"# Prepare to plunder the data for valuable triplets!\ntriplets = []\ndocument, token, label, token_str = [], [], [], []\n\n# For each prediction, token mapping, offsets, tokens, and document in the dataset\nfor p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    # Iterate through each token prediction and its corresponding offsets\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]  # Predicted label from token\n\n        # If start and end indices sum to zero, continue to the next iteration\n        if start_idx + end_idx == 0:\n            continue\n\n        # If the token mapping at the start index is -1, increment start index\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # Ignore leading whitespace tokens (\"\\n\\n\")\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        # If start index exceeds the length of token mapping, break the loop\n        if start_idx >= len(token_map):\n            break\n\n        token_id = token_map[start_idx]  # Token ID at start index\n\n        # Ignore \"O\" predictions and whitespace tokens\n        if label_pred != \"O\" and token_id != -1:\n            triplet = (label_pred, token_id, tokens[token_id])  # Form a triplet\n\n            # If the triplet is not in the list of triplets, add it\n            if triplet not in triplets:\n                document.append(doc)\n                token.append(token_id)\n                label.append(label_pred)\n                token_str.append(tokens[token_id])\n                triplets.append(triplet)\n\n# We've gathered the valuable triplets from the dataset, ready for analysis!\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:08.983581Z","iopub.execute_input":"2024-04-18T23:03:08.983910Z","iopub.status.idle":"2024-04-18T23:03:09.085844Z","shell.execute_reply.started":"2024-04-18T23:03:08.983877Z","shell.execute_reply":"2024-04-18T23:03:09.085108Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Haul in the data and prepare for your quest!\ndf = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\n\n# Assign each row a unique 'row_id'\ndf[\"row_id\"] = list(range(len(df)))\n\n# Display a glimpse of the first 100 rows of your data\ndisplay(df.head(100))\n\n# Cast your findings into a CSV file for further exploration\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n\n# May the winds of fortune guide ye to untold discoveries!\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:09.086878Z","iopub.execute_input":"2024-04-18T23:03:09.087151Z","iopub.status.idle":"2024-04-18T23:03:09.125817Z","shell.execute_reply.started":"2024-04-18T23:03:09.087126Z","shell.execute_reply":"2024-04-18T23:03:09.125056Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"    document  token           label   token_str  row_id\n0          7      9  B-NAME_STUDENT    Nathalie       0\n1          7     10  I-NAME_STUDENT       Sylla       1\n2          7    482  B-NAME_STUDENT    Nathalie       2\n3          7    483  I-NAME_STUDENT       Sylla       3\n4          7    741  B-NAME_STUDENT    Nathalie       4\n5          7    742  I-NAME_STUDENT       Sylla       5\n6         10      0  B-NAME_STUDENT       Diego       6\n7         10      1  I-NAME_STUDENT     Estrada       7\n8         10    464  B-NAME_STUDENT       Diego       8\n9         10    465  I-NAME_STUDENT     Estrada       9\n10        16      4  B-NAME_STUDENT    Gilberto      10\n11        16      5  I-NAME_STUDENT      Gamboa      11\n12        20      5  B-NAME_STUDENT       Sindy      12\n13        20      6  I-NAME_STUDENT      Samaca      13\n14        56     12  B-NAME_STUDENT      Nadine      14\n15        56     13  I-NAME_STUDENT        Born      15\n16        86      6  B-NAME_STUDENT      Eladio      16\n17        86      7  I-NAME_STUDENT       Amaya      17\n18        93      0  B-NAME_STUDENT      Silvia      18\n19        93      1  I-NAME_STUDENT  Villalobos      19\n20       104      8  B-NAME_STUDENT       Sakir      20\n21       104      9  I-NAME_STUDENT       Ahmad      21\n22       112      5  B-NAME_STUDENT   Francisco      22\n23       112      6  I-NAME_STUDENT    Ferreira      23\n24       123     32  B-NAME_STUDENT     Stefano      24\n25       123     33  I-NAME_STUDENT      Lovato      25","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.metrics import f1_score\n# Convert predictions to actual labels (not probabilities)\npredicted_labels = np.argmax(weighted_average_predictions, axis=-1)\n# Calculate the accuracy\naccuracy = np.mean(predicted_labels == labels)\nprint(f\"Model accuracy: {accuracy * 100:.2f}%\")\nbert_f1_score = f1_score(labels, predicted_labels, average='micro')\nprint(f\"Model F1 score: {bert_f1_score}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-18T23:03:09.126960Z","iopub.execute_input":"2024-04-18T23:03:09.127347Z","iopub.status.idle":"2024-04-18T23:03:09.134177Z","shell.execute_reply.started":"2024-04-18T23:03:09.127269Z","shell.execute_reply":"2024-04-18T23:03:09.133270Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'\\nfrom sklearn.metrics import f1_score\\n# Convert predictions to actual labels (not probabilities)\\npredicted_labels = np.argmax(weighted_average_predictions, axis=-1)\\n# Calculate the accuracy\\naccuracy = np.mean(predicted_labels == labels)\\nprint(f\"Model accuracy: {accuracy * 100:.2f}%\")\\nbert_f1_score = f1_score(labels, predicted_labels, average=\\'micro\\')\\nprint(f\"Model F1 score: {bert_f1_score}\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# predicted_labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:10:56.188104Z","iopub.status.idle":"2024-04-18T18:10:56.188555Z","shell.execute_reply.started":"2024-04-18T18:10:56.188314Z","shell.execute_reply":"2024-04-18T18:10:56.188334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# log models:","metadata":{}},{"cell_type":"code","source":"# WITH AUGMENTED TRAINING DATA\n\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import f1_score\n\n# Load training data\nwith open(\"/kaggle/input/pii-data-207-aug/pii_train_format2.json\", 'r') as file:\n    train_data = json.load(file)\n\n# Extract features and labels\ntrain_documents, train_tokens, train_labels, train_token_strs = [], [], [], []\nfor entry in train_data:\n    doc_id = entry['document']\n    for token, label, trailing_space in zip(entry['tokens'], entry['labels'], entry['trailing_whitespace']):\n        train_documents.append(doc_id)\n        train_tokens.append(token)\n        train_labels.append(label)\n        train_token_strs.append(token + \" \" if trailing_space else token)\n\ntrain_df = pd.DataFrame({\n    \"document\": train_documents,\n    \"token\": train_tokens,\n    \"label\": train_labels,\n    \"token_str\": train_token_strs\n})\n\n# Duplicate rows where the label is only 1 I-ID_NUM (for stratify to work in train_test_split)\ndup_1 = train_df[train_df['label'] == 'I-URL_PERSONAL']\ndup_2 = train_df[train_df['label'] == 'I-ID_NUM']\ntrain_df = pd.concat([train_df, dup_1, dup_2], ignore_index=True)\n\n# Split the dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(train_df['token_str'], train_df['label'], test_size=0.2, random_state=42, stratify=train_df['label'])\n\n# Create a pipeline with TF-IDF Vectorizer and Logistic Regression\npipeline = make_pipeline(TfidfVectorizer(max_features=1000), LogisticRegression(max_iter=1000))\n\nprint(\"Start Training:\")\n# Train the model on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the training and validation data\ny_train_pred = pipeline.predict(X_train)\ny_val_pred = pipeline.predict(X_val)\n\n# Calculate and print accuracy for training and validation sets\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nprint(f\"Training Accuracy: {train_accuracy}\")\nprint(f\"Validation Accuracy: {val_accuracy}\")\ntrain_f1 = f1_score(y_train, y_train_pred, average='micro')\nval_f1 = f1_score(y_val, y_val_pred, average='micro')\nprint(f\"Training F1: {train_f1}\")\nprint(f\"Validation F1: {val_f1}\")\n\n\n\n\n# Prepare the submission using test data\n# Load test data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", 'r') as file:\n    test_data = json.load(file)\n\n# Extract features for test data\ntest_documents, test_tokens, test_token_strs = [], [], []\nfor entry in test_data:\n    doc_id = entry['document']\n    for token, trailing_space in zip(entry['tokens'], entry['trailing_whitespace']):\n        test_documents.append(doc_id)\n        test_tokens.append(token)\n        test_token_strs.append(token + \" \" if trailing_space else token)\n\ntest_df = pd.DataFrame({\n    \"document\": test_documents,\n    \"token_str\": test_token_strs\n})\n\n# Use the trained pipeline to predict labels for the test data\ntest_df['label'] = pipeline.predict(test_df['token_str'])\n\n# Filter predictions to include only positive PII labels and prepare for submission\nsubmission_df = test_df[test_df['label'] != 'O'].copy()\n# submission_df = test_df.copy()\nsubmission_df.reset_index(drop=True, inplace=True)\nsubmission_df['row_id'] = submission_df.index\nformatted_submission = submission_df[['row_id', 'document', 'token_str', 'label']]\n\n# Save the submission file\nformatted_submission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file 'submission.csv' prepared.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T01:51:40.955579Z","iopub.execute_input":"2024-04-19T01:51:40.955905Z","iopub.status.idle":"2024-04-19T04:30:12.848028Z","shell.execute_reply.started":"2024-04-19T01:51:40.955879Z","shell.execute_reply":"2024-04-19T04:30:12.846478Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Start Training:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Make predictions on the training and validation data\u001b[39;00m\n\u001b[1;32m     49\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_train)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1291\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    446\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    447\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    448\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    449\u001b[0m ]\n\u001b[0;32m--> 450\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    459\u001b[0m     solver,\n\u001b[1;32m    460\u001b[0m     opt_res,\n\u001b[1;32m    461\u001b[0m     max_iter,\n\u001b[1;32m    462\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_minimize.py:623\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    621\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    627\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/lbfgsb.py:360\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    354\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:233\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:134\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/optimize.py:74\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/optimize.py:68\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 68\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:278\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m--> 278\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    285\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/_loss/loss.py:257\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m ReadonlyArrayWrapper(sample_weight)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"dcdc","metadata":{}},{"cell_type":"code","source":"# WITH CROSS VALIDATION\n'''\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import ParameterGrid\n\n# Load training data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\", 'r') as file:\n    train_data = json.load(file)\n\n# Extract features and labels\ntrain_documents, train_tokens, train_labels, train_token_strs = [], [], [], []\nfor entry in train_data:\n    doc_id = entry['document']\n    for token, label, trailing_space in zip(entry['tokens'], entry['labels'], entry['trailing_whitespace']):\n        train_documents.append(doc_id)\n        train_tokens.append(token)\n        train_labels.append(label)\n        train_token_strs.append(token + \" \" if trailing_space else token)\n\ntrain_df = pd.DataFrame({\n    \"document\": train_documents,\n    \"token\": train_tokens,\n    \"label\": train_labels,\n    \"token_str\": train_token_strs\n})\n\n# Split the dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(train_df['token_str'], train_df['label'], test_size=0.2, random_state=42)\n\n# Define the hyperparameters grid\nparams = {\n    \"max_features\": [1000, 5000, 10000]\n}\n\nbest_model = None\nbest_score = 0\nbest_param = None\n\n#Perform grid search\nfor param in ParameterGrid(params):\n    # Create a pipeline with TF-IDF Vectorizer and Logistic Regression\n    pipeline = make_pipeline(TfidfVectorizer(max_features=param[\"max_features\"]), LogisticRegression(max_iter=1000))\n\n    print(\"Start Training with param {}:\".format(param[\"max_features\"]))\n    # Train the model on the training data\n    pipeline.fit(X_train, y_train)\n\n    # Make predictions on the training and validation data\n    y_train_pred = pipeline.predict(X_train)\n    y_val_pred = pipeline.predict(X_val)\n\n    # Calculate and print accuracy for training and validation sets\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    val_accuracy = accuracy_score(y_val, y_val_pred)\n    print(f\"Training Accuracy: {train_accuracy}\")\n    print(f\"Validation Accuracy: {val_accuracy}\")\n    train_f1 = f1_score(y_train, y_train_pred, average='micro')\n    val_f1 = f1_score(y_val, y_val_pred, average='micro')\n    print(f\"Training F1: {train_f1}\")\n    print(f\"Validation F1: {val_f1}\\n\")\n    if val_f1 > best_score:\n        best_score = val_f1\n        best_model = pipeline\n        best_param = param[\"max_features\"]\n\nprint(f\"Best Validation F1: {best_score}\")\nprint(f\"Best max_features param: {best_param}\")\n\n\n# Prepare the submission using test data\n# Load test data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", 'r') as file:\n    test_data = json.load(file)\n\n# Extract features for test data\ntest_documents, test_tokens, test_token_strs = [], [], []\nfor entry in test_data:\n    doc_id = entry['document']\n    for token, trailing_space in zip(entry['tokens'], entry['trailing_whitespace']):\n        test_documents.append(doc_id)\n        test_tokens.append(token)\n        test_token_strs.append(token + \" \" if trailing_space else token)\n\ntest_df = pd.DataFrame({\n    \"document\": test_documents,\n    \"token_str\": test_token_strs\n})\n\n# Use the best trained model to predict labels for the test data\ntest_df['label'] = best_model.predict(test_df['token_str'])\n\n# Filter predictions to include only positive PII labels and prepare for submission\nsubmission_df = test_df[test_df['label'] != 'O'].copy()\nsubmission_df.reset_index(drop=True, inplace=True)\nsubmission_df['row_id'] = submission_df.index\nformatted_submission = submission_df[['row_id', 'document', 'token_str', 'label']]\n\n# Save the submission file\nformatted_submission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file 'submission.csv' prepared.\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:43:46.377607Z","iopub.execute_input":"2024-04-18T18:43:46.377987Z","iopub.status.idle":"2024-04-18T19:04:02.245598Z","shell.execute_reply.started":"2024-04-18T18:43:46.377940Z","shell.execute_reply":"2024-04-18T19:04:02.244635Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Start Training with param 1000:\nTraining Accuracy: 0.9994484262245664\nValidation Accuracy: 0.9994631985554433\nTraining F1: 0.9994484262245664\nValidation F1: 0.9994631985554433\n\nStart Training with param 5000:\nTraining Accuracy: 0.999459943425506\nValidation Accuracy: 0.99947020902207\nTraining F1: 0.999459943425506\nValidation F1: 0.99947020902207\n\nStart Training with param 10000:\nTraining Accuracy: 0.9994964980197926\nValidation Accuracy: 0.9995002538790414\nTraining F1: 0.9994964980197926\nValidation F1: 0.9995002538790414\n\nBest Validation F1: 0.9995002538790414\nBest max_features param: 10000\nSubmission file 'submission.csv' prepared.\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import f1_score\n\n# Load training data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\", 'r') as file:\n    train_data = json.load(file)\n\n# Extract features and labels\ntrain_documents, train_tokens, train_labels, train_token_strs = [], [], [], []\nfor entry in train_data:\n    doc_id = entry['document']\n    for token, label, trailing_space in zip(entry['tokens'], entry['labels'], entry['trailing_whitespace']):\n        train_documents.append(doc_id)\n        train_tokens.append(token)\n        train_labels.append(label)\n        train_token_strs.append(token + \" \" if trailing_space else token)\n\ntrain_df = pd.DataFrame({\n    \"document\": train_documents,\n    \"token\": train_tokens,\n    \"label\": train_labels,\n    \"token_str\": train_token_strs\n})\n\n# Duplicate rows where the label is only 1 I-ID_NUM (for stratify to work in train_test_split)\ndup_1 = train_df[train_df['label'] == 'I-URL_PERSONAL']\ndup_2 = train_df[train_df['label'] == 'I-ID_NUM']\ntrain_df = pd.concat([train_df, dup_1, dup_2], ignore_index=True)\n\n# Split the dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(train_df['token_str'], train_df['label'], test_size=0.2, random_state=42, stratify=train_df['label'])\n\n# Create a pipeline with TF-IDF Vectorizer and Logistic Regression\npipeline = make_pipeline(TfidfVectorizer(max_features=1000), LogisticRegression(max_iter=1000))\n\nprint(\"Start Training:\")\n# Train the model on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the training and validation data\ny_train_pred = pipeline.predict(X_train)\ny_val_pred = pipeline.predict(X_val)\n\n# Calculate and print accuracy for training and validation sets\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nprint(f\"Training Accuracy: {train_accuracy}\")\nprint(f\"Validation Accuracy: {val_accuracy}\")\ntrain_f1 = f1_score(y_train, y_train_pred, average='micro')\nval_f1 = f1_score(y_val, y_val_pred, average='micro')\nprint(f\"Training F1: {train_f1}\")\nprint(f\"Validation F1: {val_f1}\")\n\n\n\n\n# Prepare the submission using test data\n# Load test data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", 'r') as file:\n    test_data = json.load(file)\n\n# Extract features for test data\ntest_documents, test_tokens, test_token_strs = [], [], []\nfor entry in test_data:\n    doc_id = entry['document']\n    for token, trailing_space in zip(entry['tokens'], entry['trailing_whitespace']):\n        test_documents.append(doc_id)\n        test_tokens.append(token)\n        test_token_strs.append(token + \" \" if trailing_space else token)\n\ntest_df = pd.DataFrame({\n    \"document\": test_documents,\n    \"token_str\": test_token_strs\n})\n\n# Use the trained pipeline to predict labels for the test data\ntest_df['label'] = pipeline.predict(test_df['token_str'])\n\n# Filter predictions to include only positive PII labels and prepare for submission\nsubmission_df = test_df[test_df['label'] != 'O'].copy()\n# submission_df = test_df.copy()\nsubmission_df.reset_index(drop=True, inplace=True)\nsubmission_df['row_id'] = submission_df.index\nformatted_submission = submission_df[['row_id', 'document', 'token_str', 'label']]\n\n# Save the submission file\nformatted_submission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file 'submission.csv' prepared.\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:37:10.715907Z","iopub.execute_input":"2024-04-18T19:37:10.716322Z","iopub.status.idle":"2024-04-18T19:43:19.968980Z","shell.execute_reply.started":"2024-04-18T19:37:10.716285Z","shell.execute_reply":"2024-04-18T19:43:19.968066Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Start Training:\nTraining Accuracy: 0.9994509302388466\nValidation Accuracy: 0.9994511806126547\nTraining F1: 0.9994509302388466\nValidation F1: 0.9994511806126547\nSubmission file 'submission.csv' prepared.\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:43:27.210730Z","iopub.execute_input":"2024-04-18T19:43:27.211205Z","iopub.status.idle":"2024-04-18T19:43:27.225850Z","shell.execute_reply.started":"2024-04-18T19:43:27.211164Z","shell.execute_reply":"2024-04-18T19:43:27.224882Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"      document                                          token_str label\n0            7                                            Design      O\n1            7                                          Thinking      O\n2            7                                               for      O\n3            7                                        innovation      O\n4            7                                          reflexion     O\n...        ...                                                ...   ...\n8500       123                                                 (      O\n8501       123                          https://www.melessa.uni-      O\n8502       123  muenchen.de/team/vorstandssprecher/schmidt/pub...     O\n8503       123                                                  )     O\n8504       123                                               \\n\\n     O\n\n[8505 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token_str</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>Design</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>Thinking</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>for</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>innovation</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>reflexion</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8500</th>\n      <td>123</td>\n      <td>(</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8501</th>\n      <td>123</td>\n      <td>https://www.melessa.uni-</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8502</th>\n      <td>123</td>\n      <td>muenchen.de/team/vorstandssprecher/schmidt/pub...</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8503</th>\n      <td>123</td>\n      <td>)</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8504</th>\n      <td>123</td>\n      <td>\\n\\n</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>8505 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# test_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:43:29.643561Z","iopub.execute_input":"2024-04-18T19:43:29.644475Z","iopub.status.idle":"2024-04-18T19:43:29.654860Z","shell.execute_reply.started":"2024-04-18T19:43:29.644426Z","shell.execute_reply":"2024-04-18T19:43:29.653931Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"label\nO    8505\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# train_df['label'].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:23:59.253376Z","iopub.execute_input":"2024-04-18T19:23:59.253761Z","iopub.status.idle":"2024-04-18T19:23:59.262192Z","shell.execute_reply.started":"2024-04-18T19:23:59.253729Z","shell.execute_reply":"2024-04-18T19:23:59.261163Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0    O\n1    O\n2    O\n3    O\n4    O\nName: label, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# train_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:32:45.329960Z","iopub.execute_input":"2024-04-18T19:32:45.330838Z","iopub.status.idle":"2024-04-18T19:32:45.923320Z","shell.execute_reply.started":"2024-04-18T19:32:45.330803Z","shell.execute_reply":"2024-04-18T19:32:45.922189Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"label\nO                   4989794\nB-NAME_STUDENT         1365\nI-NAME_STUDENT         1096\nB-URL_PERSONAL          110\nB-ID_NUM                 78\nB-EMAIL                  39\nI-STREET_ADDRESS         20\nI-PHONE_NUM              15\nB-USERNAME                6\nB-PHONE_NUM               6\nB-STREET_ADDRESS          2\nI-URL_PERSONAL            1\nI-ID_NUM                  1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# train_df[train_df['label'] == 'I-URL_PERSONAL']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:34:01.290418Z","iopub.execute_input":"2024-04-18T19:34:01.291404Z","iopub.status.idle":"2024-04-18T19:34:02.118545Z","shell.execute_reply.started":"2024-04-18T19:34:01.291359Z","shell.execute_reply":"2024-04-18T19:34:02.117380Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"       document      token           label  token_str\n83770      3202  nYZqnhEXw  I-URL_PERSONAL  nYZqnhEXw","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83770</th>\n      <td>3202</td>\n      <td>nYZqnhEXw</td>\n      <td>I-URL_PERSONAL</td>\n      <td>nYZqnhEXw</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}